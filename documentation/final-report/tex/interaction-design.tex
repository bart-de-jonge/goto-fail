\section{Interaction Design}
While designing our application, we performed several user tests. The user testing aims to improve the interaction of the application with the user. Most of the time, an application is made and the interaction between the user and application is far from optimal. Developers often forget that they are not the ones using the application when it is done. Some things might be clear to the developer, but not to the user. We tried to improve this by testing our application with the users during development. This kind of testing can greatly improve the way users interact with your application and results in a better end-product.
\subsection{Method}
All applications are tying to solve a specific problem. When an application has good interaction design, the way the user solves this problem is easy and clear. We want to ensure that the user is able to easily solve the problem or perform a task. It should not be the case that a user gets lost in the application and just gives up. We would like to know what makes an application user friendly. What is bad to do, what is good to do. The way the users use your application depends greatly on the way the users interact with it. If users dislike the user experience, they will not use it very much. You want to tailor the interface of the application to fit the needs of the user. In this manner, the user is able to interact with it in a more intuitive way.\\

Our application consists of multiple parts, firstly a scripting application and secondly a web server. Both parts of our application required user testing. The one that needed the most testing is the scripting application, since the other part is a web view that follows the specification of Google's material design. A lot of thought has gone into material design, so we thought that we needed to place emphasis on the scripting application for our user tests. We started summing up the main tasks that we wanted a user to be able to perform while using our application. This was the core of our user test. The core consists of adding shots to a project, checking collisions between all the shots and modifying the shots. Of course, there were additional features, but these actions form the foundation of the workflow. Once we had all this information, we started filling in the gaps between these actions. Some actions require preceding tasks be performed in order to complete the action itself. For instance, when adding a shot, you first have to create a project. When these gaps were filled in, we were able to set up a document with the instructions for the user, for use during the test. We looked specifically for the things that a user would expect to see in the application. If items are where the user expects them to be, then the positioning is correct. We also wanted to know what was going on inside the head of the user while working with the application. As such, we asked the test subjects to use the think aloud protocol. We were then able to hear what the user thought about our application.\\

Our test subjects were similar to the end users of the product. Because of this criterion, we chose to use members from another context project group, from the same context. The group members were familiar with all of the processes within Polycast and during a production. We did not want to have people completely unfamiliar with the product domain working with our application, as this would result in suboptimal changes. The target audience has experience with cameras, scripting and movie/concert productions.
\subsection{Results}
Upon opening the application, the user is presented with a choice: create a new project or open an existing one. The users were surprised by the choice and proceeded to create a new project. Upon creating a new project, the users found the division between the fields and boxes to be clean and ordered. One of the users assumed that the description field was optional. It was not completely clear what seconds per count meant. It was also annoying that the text was cut off with ellipsis, and this could be found throughout the entire scripting application. In addition, it was not clear that a camera type had to be created before creating cameras.\\

When creating new shots, description was required, and users found this to be a nuisance. It was unclear what start and end meant, and it was also unclear what the type/format of the input should be. The rest of the process fir adding a shot was very clear and intuitive. A bug was found that occurs when a user clicks on a shot. There was a small chance that the shot could collapse, which was unexpected. On the other hand, dragging worked really well, it is very clear and easy to use. The ability to resize a shot was also clear and intuitive. When changing information in the shot, it was very nice to see it update immediately! Users found that adding a directorshot was almost the same as adding a camerashot, except that the new fields, padding both before and after, had no clear meaning. The generate all shots button was easy to locate. Users immediately noticed when two shots are colliding. Because of the dragging functionality all of the collisions were easily repaired. One caveat was that users found it strange that there was no visible padding before and after each shot. Changing the color of the application was also intuitive, and the button was where they expected it to be. Uploading the project to the webserver was also accomplished without any issues.\\

On the web server, navigating was easy, as all of the pages are located in the side-bar. Opening the detail view of a shot was also intuitive, as it is clickable. It also disappears after some time. However, when trying to advance the current count in the shot-caller view, it was not completely clear how this could be accomplished. The buttons did not look like buttons, which was another point for improvement.
\subsection{Conclusion and Discussion}
During the user tests we gathered good feedback. Because we have worked on the application for quite some time, we often overlooked the simple stuff that could be unclear for the average user. An example of this is the text that is too long for the space it has. Users need clear information, not information that is missing or cut off. Some things are really basic, like the appearance of the button. These are small but easily fixable issues. Most of these issues are already fixed, but the more complicated problems require a bit more time to resolve. In conclusion, the user tests were really handy, and the feedback was quite useful. Next time, we would plan multiple of these sessions. Since an application is ever-changing, user testing should also be performed multiple times. We also would have wanted to have a real end-user test our project, but receiving feedback or even a reply was difficult. If a session with the real end-user were to take place, the feedback would be even more useful, since they will have/want to work with the application on a daily basis. Since this was not possible, we are satisfied with the way our user tests turned out.
